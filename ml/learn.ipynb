{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "37emi23szdssau0nque5o8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "# import torchtext\n",
    "import torch_geometric\n",
    "from tqdm import tqdm\n",
    "import identify_x86_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "h3gyr0cmlyp73wob8u32j",
    "execution_id": "9bcf8f7f-fdb4-4f02-9716-0aad31c04472"
   },
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from identify_x86_graph import load_graph, load_split\n",
    "\n",
    "# simplify debugging out-of-memory conditions by serializing all graph loading and printing graph filenames\n",
    "# this can give you an idea of which graphs are too large and need to be excluded\n",
    "FIX_MEM = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "e1d3wiuf9v7cyl2rgi7ao"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# TODO: use PyG's Dataset class\n",
    "\n",
    "# TODO: now that we have a list of superset files we should be able to create a dataset that caches the graph conversion\n",
    "\n",
    "from torch_geometric.data import Dataset\n",
    "from torch.utils.data import random_split, Subset\n",
    "\n",
    "class IdentifyDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        root = root.rstrip('/')\n",
    "        super().__init__(root, transform)\n",
    "        self.vocab = open(os.path.join(root, 'raw/code.vocab')).read().splitlines()\n",
    "        self.split = load_split(os.path.join(root, 'raw/split.txt'))\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # find all .graph files in the root\n",
    "        return glob.glob('**/*.graph', root_dir=os.path.join(self.root, 'raw'), recursive=True)\n",
    "\n",
    "    @property\n",
    "    def program_names(self):\n",
    "        return [ self.path_to_name(path) for path in self.raw_paths ]\n",
    "\n",
    "    @property\n",
    "    def splits(self):\n",
    "        return [ self.split[name] for name in self.program_names ]\n",
    "\n",
    "    def split_indices(self, split):\n",
    "        return [i for i, s in enumerate(self.splits) if s == split]\n",
    "    \n",
    "    def split_names(self, split):\n",
    "        return [self.program_names[i] for i in self.split_indices(split)]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'{f}.pt' for f in self.raw_file_names]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        pass\n",
    "\n",
    "    def path_to_name(self, path):\n",
    "        return path.removesuffix('.graph').removeprefix(self.root).removeprefix('/raw/')\n",
    "\n",
    "    def get(self, idx):\n",
    "        path = self.raw_paths[idx]\n",
    "        name = self.path_to_name(path)\n",
    "        if FIX_MEM:\n",
    "            print(\"Loading\", name)\n",
    "        data = load_graph(path, name=name)\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        return data\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "dataset = IdentifyDataset('data')\n",
    "\n",
    "# print(dataset.program_names)\n",
    "# print(dataset.splits)\n",
    "# print(*dataset.split_names('test'), sep='\\n')\n",
    "\n",
    "test_dataset = Subset(dataset, dataset.split_indices('test'))\n",
    "train_dataset = Subset(dataset, dataset.split_indices('train'))\n",
    "\n",
    "# test_dataset, train_dataset = random_split(\n",
    "#     dataset, \n",
    "#     [0.1, 0.9], generator=torch.Generator().manual_seed(42)\n",
    "# )\n",
    "VOCAB_SIZE = len(dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert VOCAB_SIZE == 502 # this is hard-coded in identify_x86_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(name='byteweight/elf-x86/gcc_coreutils_32_O3_make-prime-list', num_nodes=5640, x_code=[5640], x_size=[5640], y=[5640], num_edges=52420, edge_index=[2, 52420], edge_type=[52420])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_size(dataset):\n",
    "    return sum([ data.num_nodes for data in dataset ])\n",
    "\n",
    "\n",
    "# test_size = dataset_size(test_dataset)\n",
    "# train_size = dataset_size(train_dataset)\n",
    "# print(\"Test dataset size:\", test_size)\n",
    "# print(\"Train dataset size:\", train_size)\n",
    "# print(\"Test proportion:\", test_size / (train_size + test_size))\n",
    "# print(\"Train proportion:\", train_size / (train_size + test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "ca5zenizst64jpx3g3xjjk"
   },
   "outputs": [],
   "source": [
    "from identify_x86_model import LightningModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "npknzk750bkdrcncavnsh"
   },
   "outputs": [],
   "source": [
    "model = LightningModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "fknkqidhgskcld5y9r6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LightningModel(\n",
       "  (model): IdentifyModel(\n",
       "    (model): Sequential(\n",
       "      (0): Embedding(15, 4)\n",
       "      (1): Embedding(502, 32)\n",
       "      (2): <torch.jit.ScriptFunction object at 0x7f07bbb78130>\n",
       "      (3): RGCNConvJittable_906d57(36, 24, num_relations=7)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): RGCNConvJittable_907592(24, 16, num_relations=7)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): RGCNConvJittable_907c9f(16, 8, num_relations=7)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): RGCNConvJittable_9083ad(8, 4, num_relations=7)\n",
       "      (10): ReLU(inplace=True)\n",
       "      (11): Linear(4, 2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       "  (train_accuracy): BinaryAccuracy()\n",
       "  (train_precision): BinaryPrecision()\n",
       "  (train_recall): BinaryRecall()\n",
       "  (train_f1): BinaryF1Score()\n",
       "  (valid_accuracy): BinaryAccuracy()\n",
       "  (valid_precision): BinaryPrecision()\n",
       "  (valid_recall): BinaryRecall()\n",
       "  (valid_f1): BinaryF1Score()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "ot8f00jygigt8ah5r18mo",
    "execution_id": "cdda3a2f-0c60-41b6-bddf-aef11e9f5185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0 | model           | IdentifyModel    | 27.4 K\n",
      "1 | loss            | CrossEntropyLoss | 0     \n",
      "2 | train_accuracy  | BinaryAccuracy   | 0     \n",
      "3 | train_precision | BinaryPrecision  | 0     \n",
      "4 | train_recall    | BinaryRecall     | 0     \n",
      "5 | train_f1        | BinaryF1Score    | 0     \n",
      "6 | valid_accuracy  | BinaryAccuracy   | 0     \n",
      "7 | valid_precision | BinaryPrecision  | 0     \n",
      "8 | valid_recall    | BinaryRecall     | 0     \n",
      "9 | valid_f1        | BinaryF1Score    | 0     \n",
      "-----------------------------------------------------\n",
      "27.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "27.4 K    Total params\n",
      "0.110     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134:  84%|████████▍ | 773/918 [04:41<00:52,  2.74it/s, v_num=63]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dcnick3/.conda/envs/ml2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "print(\"Cuda is available:\", torch.cuda.is_available())\n",
    "\n",
    "# enable medium precision to utilize tensor cores\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "if FIX_MEM:\n",
    "    num_workers = 0\n",
    "else:\n",
    "    num_workers = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=None, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=None, shuffle=False, num_workers=0)\n",
    "val_loader = DataLoader(test_dataset, batch_size=None, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "model = LightningModel()\n",
    "num_epochs = 2000\n",
    "# num_epochs = 20\n",
    "# val_check_interval = len(train_loader)\n",
    "\n",
    "checkpoint_callback_best_f1 = ModelCheckpoint(\n",
    "    monitor = 'f1/val',\n",
    "    mode = 'max',\n",
    "    filename = 'best-f1={f1/val:.8f}-epoch={epoch}',\n",
    "    save_top_k = 4,\n",
    "    auto_insert_metric_name = False,\n",
    "    save_last = True,\n",
    ")\n",
    "checkpoint_callback_all = ModelCheckpoint(\n",
    "    filename = 'all-{epoch}',\n",
    "    save_top_k = -1,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # enable mixed precision training\n",
    "    precision='16-mixed',\n",
    "    max_epochs = num_epochs,\n",
    "    # val_check_interval = val_check_interval,\n",
    "    log_every_n_steps = 1,\n",
    "    accelerator = 'gpu',\n",
    "    # accelerator = 'cpu',\n",
    "    enable_progress_bar = not FIX_MEM,\n",
    "    enable_checkpointing = True,\n",
    "\n",
    "    callbacks = [\n",
    "        checkpoint_callback_best_f1,\n",
    "        checkpoint_callback_all,\n",
    "    ]\n",
    ")\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2764,  0.1149,  0.7211,  0.0184],\n",
       "        [-0.1471,  0.2820, -0.1687,  0.0621]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.model[-1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "notebookId": "a155bcf5-968b-4cf3-90b6-eed71f0d9eb7",
  "notebookPath": "learn.ipynb",
  "vscode": {
   "interpreter": {
    "hash": "b6f9007b797bfe77ea54e56ef8f71f2cbe93648552f308c16523f111525d0f28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
